---
title: "Modeling"
format: html
editor: visual
---

## Introduction

## Imports and Full Recipe

```{r}
source("./Scripts/data_load.R")
library(tidymodels)
```

```{r}
set.seed(558)

diabetes_split <- initial_split(diabetes_tbl, prop = .7)
diabetes_cv_folds <- vfold_cv(training(diabetes_split), 5)

# we'll standardize BMI and dummy everything else

diabetes_recipe <- recipe(DiabetesOutcome ~ ., data = training(diabetes_split)) |> 
  step_normalize(BMI) |> 
  step_dummy(all_nominal_predictors())
```

## Classification Tree

A classification tree is a model that produces a prediction for each path through a tree of decisions based on predictor variable values. The number of decision points can be chosen as a model parameter. Here, for example, the model could split on whether the individual is a smoker, and all outputs will be either "Yes" or "No" to represent DiabetesOutput.

```{r}
ctree_spec <- decision_tree(tree_depth = tune(), min_n = 20, cost_complexity = tune()) |> 
  set_engine("rpart") |> 
  set_mode("classification")

ctree_wf <- workflow() |> 
  add_recipe(diabetes_recipe) |> 
  add_model(ctree_spec)

ctree_grid <- grid_regular(parameters(ctree_spec), levels = 5)

ctree_cv_run <- ctree_wf |> 
  tune_grid(resamples = diabetes_cv_folds, grid = ctree_grid, metrics = metric_set(mn_log_loss))

ctree_best_params <- ctree_cv_run |> 
  select_best(metric = "mn_log_loss")

saveRDS(ctree_cv_run, file = "./Saved Objects/ctree_cv_run.RDS")
```

## Random Forest

```{r}
rf_spec <- rand_forest(mtry = tune()) |>
set_engine("ranger",importance = "impurity") |>
set_mode("classification")

rf_wf <- workflow() |> 
  add_recipe(diabetes_recipe) |> 
  add_model(rf_spec)

rf_cv_run <- rf_wf |> 
  tune_grid(resamples = diabetes_cv_folds, grid = 5, metrics = metric_set(mn_log_loss))

rf_best_params <- rf_cv_run |> 
  select_best(metric = "mn_log_loss")

saveRDS(rf_cv_run, file = "./Saved Objects/rf_cv_run.RDS")
```

## Comparison

```{r}
ctree_wf |> 
  finalize_workflow(ctree_best_params) |> 
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss)) |> 
  collect_metrics()

rf_wf |> 
  finalize_workflow(rf_best_params) |> 
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss)) |> 
  collect_metrics()
```

Going purely by minimizing log-loss, the random forest performs slightly better.

```{r}
rf_best_fit <- rf_wf |> 
  finalize_workflow(rf_best_params) |> 
  fit(training(diabetes_split))

saveRDS(rf_best_fit, file="./Saved Objects/rf_best_wf.RDS")
```

